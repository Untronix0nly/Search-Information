стандарт исключений для роботов robots txt файл ограничения доступа роботам содержимому на http сервере файл должен находиться корне сайта то есть иметь путь относительно имени сайта robots txt при наличии нескольких поддоменов файл должен располагаться корневом каталоге каждого из них использование файла добровольно стандарт был принят консорциумом января года списке рассылки robots request nexor co uk тех пор используется большинством известных поисковых машин файл robots txt используется для частичного управления индексированием сайта поисковыми роботами этот файл состоит из набора инструкций для поисковых машин при помощи которых можно задать файлы страницы или каталоги сайта которые не должны индексироваться описание структуры файл состоит из записей записи разделяются одной или более пустых строк признак конца строки символы cr cr lf lf каждая запись содержит непустые строки следующего вида где поле это либо user agent либо disallow директиве user agent указываются роботы которые должны следовать указанным инструкциям например user agent yandex user agent yandexbot user agent сравнение производится методом простого поиска подстроки например запись disallow about запретит доступ как разделу http example com about так файлу http example com about php запись disallow about только разделу http example com about проверка синтаксиса неправильно составленный robots txt может привести отрицательным последствиям например весь сайт может выпасть из поискового индекса для проверки синтаксиса структуры файла robots txt существует ряд онлайн служб яндекс вебмастер анализ robots txt выполняет проверку синтаксиса разрешения для каждой отдельной страницы google search console инструмент проверки файла robots txt позволяет проверить разрешения для каждой отдельной страницы примеры запрет доступа всех роботов ко всему сайту user agent disallow запрет доступа определённого робота каталогу private user agent googlebot disallow private нестандартные директивы allow имеет действие обратное директиве disallow разрешает доступ определенной части ресурса поддерживается всеми основными поисковиками следующем примере разрешается доступ файлу photo html доступ поисковиков ко всей остальной информации каталоге album запрещается allow album photo html disallow album crawl delay устанавливает время которое робот должен выдерживать между загрузкой страниц если робот будет загружать страницы слишком часто это может создать излишнюю нагрузку на сервер впрочем современные поисковые машины по умолчанию задают достаточную задержку секунды на данный момент эта директива не учитывается googlebot user agent crawl delay sitemap расположение файлов sitemaps которые могут показать что именно нужно поисковому роботу sitemap расширенный стандарт году был предложен расширенный стандарт robots txt включающий такие директивы как request rate visit time например user agent disallow downloads request rate загружать не более одной страницы за пять секунд visit time загружать страницы только промежуток утра до по гринвичу см также noindex favicon ico ссылки standard for robot exclusion robotstxt org ru файле robots txt роботах рунете по русски файлах robots txt cправка google использование robots txt помощь яндекса использование robots txt помощь mail ru категория поисковые системы